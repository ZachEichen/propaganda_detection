{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.naive_bayes import GaussianNB \n",
    "from sklearn.metrics import precision_recall_fscore_support,accuracy_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>average_tone</th>\n",
       "      <th>source_name</th>\n",
       "      <th>propaganda_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Et tu, Rhody?  A recent editorial in the Providence Journal cataloged everything it could find wrong with Connecticut and ended with this suggesti...</td>\n",
       "      <td>-3.181818</td>\n",
       "      <td>The Hartford Courant</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A recent post in The Farmington Mirror — our town’s version of The Onion — encouraged parents to take advantage of a shuttle service offered by th...</td>\n",
       "      <td>-0.424328</td>\n",
       "      <td>The Hartford Courant</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>President Donald Trump, as he often does while responding to natural disasters, mass shootings or unfolding crises, spent much of his time congrat...</td>\n",
       "      <td>-2.469136</td>\n",
       "      <td>The Hartford Courant</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>February is Black History Month, and nothing looms larger in black history than the evil specter of slavery. Three exhibits in the state take on t...</td>\n",
       "      <td>-0.894632</td>\n",
       "      <td>The Hartford Courant</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The snow was so heavy, whipped up by gusting winds, that travel was nearly impossible. Thousands abandoned their cars. Ambulances could not pass t...</td>\n",
       "      <td>-4.800000</td>\n",
       "      <td>The Hartford Courant</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35981</th>\n",
       "      <td>From The Telegraph:  Towns in Brazil have become refugee camps for a tide of desperate Venezuelans 30 AUGUST 2018 • 6:00AM Johan Rodriguez, a buil...</td>\n",
       "      <td>-3.193277</td>\n",
       "      <td>lewrockwell.com</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35982</th>\n",
       "      <td>The second episode of Consortium News on Flash Points focuses on two different perspectives on John McCain and the real meaning of Russian interfe...</td>\n",
       "      <td>0.526316</td>\n",
       "      <td>lewrockwell.com</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35983</th>\n",
       "      <td>It is beginning.  Actually, it’s been happening for a long time – like a slowly metastasizing cancer. The afflicted can no longer hide the underly...</td>\n",
       "      <td>-3.455285</td>\n",
       "      <td>lewrockwell.com</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35984</th>\n",
       "      <td>Justin’s note: As regular Dispatch readers know, every Friday we feature no-filtered insights from Doug Casey. You see, Doug isn’t just a world-cl...</td>\n",
       "      <td>-1.052049</td>\n",
       "      <td>lewrockwell.com</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35985</th>\n",
       "      <td>“No man can serve two masters” (Matthew 6:24).  The fine print at the bottom of the website where I signed up to receive some religious newsletter...</td>\n",
       "      <td>0.256410</td>\n",
       "      <td>lewrockwell.com</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>35986 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                        text  \\\n",
       "0      Et tu, Rhody?  A recent editorial in the Providence Journal cataloged everything it could find wrong with Connecticut and ended with this suggesti...   \n",
       "1      A recent post in The Farmington Mirror — our town’s version of The Onion — encouraged parents to take advantage of a shuttle service offered by th...   \n",
       "2      President Donald Trump, as he often does while responding to natural disasters, mass shootings or unfolding crises, spent much of his time congrat...   \n",
       "3      February is Black History Month, and nothing looms larger in black history than the evil specter of slavery. Three exhibits in the state take on t...   \n",
       "4      The snow was so heavy, whipped up by gusting winds, that travel was nearly impossible. Thousands abandoned their cars. Ambulances could not pass t...   \n",
       "...                                                                                                                                                      ...   \n",
       "35981  From The Telegraph:  Towns in Brazil have become refugee camps for a tide of desperate Venezuelans 30 AUGUST 2018 • 6:00AM Johan Rodriguez, a buil...   \n",
       "35982  The second episode of Consortium News on Flash Points focuses on two different perspectives on John McCain and the real meaning of Russian interfe...   \n",
       "35983  It is beginning.  Actually, it’s been happening for a long time – like a slowly metastasizing cancer. The afflicted can no longer hide the underly...   \n",
       "35984  Justin’s note: As regular Dispatch readers know, every Friday we feature no-filtered insights from Doug Casey. You see, Doug isn’t just a world-cl...   \n",
       "35985  “No man can serve two masters” (Matthew 6:24).  The fine print at the bottom of the website where I signed up to receive some religious newsletter...   \n",
       "\n",
       "       average_tone           source_name  propaganda_label  \n",
       "0         -3.181818  The Hartford Courant                 0  \n",
       "1         -0.424328  The Hartford Courant                 0  \n",
       "2         -2.469136  The Hartford Courant                 0  \n",
       "3         -0.894632  The Hartford Courant                 0  \n",
       "4         -4.800000  The Hartford Courant                 0  \n",
       "...             ...                   ...               ...  \n",
       "35981     -3.193277       lewrockwell.com                 1  \n",
       "35982      0.526316       lewrockwell.com                 1  \n",
       "35983     -3.455285       lewrockwell.com                 1  \n",
       "35984     -1.052049       lewrockwell.com                 1  \n",
       "35985      0.256410       lewrockwell.com                 1  \n",
       "\n",
       "[35986 rows x 4 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "colnames = ['text','event_location','average_tone','article_date','article_ID','article_URL_1','MBFC_factuality_label_1','article_URL','MBFC_factuality_label','URL_to_MBFC_page','source_name','MBFC_notes_about_source','MBFC_bias_label','source_URL','propaganda_label']\n",
    "train_df = pd.read_csv(\"../data_proppy/proppy_1.0.train.tsv\",sep='\\t',names=colnames)[[\"text\",'average_tone','source_name','propaganda_label']]\n",
    "eval_df = pd.read_csv(\"../data_proppy/proppy_1.0.dev.tsv\",sep='\\t',names=colnames)[[\"text\",'average_tone','source_name','propaganda_label']]\n",
    "\n",
    "train_df['propaganda_label'] = train_df['propaganda_label'].apply(lambda x: int(x==1))\n",
    "eval_df['propaganda_label'] = eval_df['propaganda_label'].apply(lambda x: int(x==1))\n",
    "\n",
    "pd.set_option('display.max_colwidth', 150)\n",
    "\n",
    "train_df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# maybe balance distribution on train, dev, test sets: \n",
    "balance_cats = True \n",
    "\n",
    "def balance_set(df_in, label_col,min_label): \n",
    "    df_out = df_in[df_in[label_col] == min_label]\n",
    "    n_to_take = df_out.shape[0]\n",
    "    for lab in [lab for lab in df_in[label_col].unique() if lab != min_label]:\n",
    "        df_out = df_out.append(df_in[df_in[label_col] == lab].sample(n=n_to_take))\n",
    "    return df_out.sample(frac=1) # randomize order \n",
    "\n",
    "if balance_cats : \n",
    "    train_df = balance_set(train_df,'propaganda_label',1)\n",
    "    eval_df = balance_set(eval_df,'propaganda_label',1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Count</th>\n",
       "      <th>frac</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>propaganda_label</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4021</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4021</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Count  frac\n",
       "propaganda_label             \n",
       "0                  4021   0.5\n",
       "1                  4021   0.5"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts = train_df[[\"source_name\",\"propaganda_label\",\"text\"]].groupby(\"propaganda_label\").count().rename(columns={\"source_name\":\"Count\",\"text\":\"frac\"})\n",
    "counts.frac = counts.frac/(counts.Count.sum())\n",
    "counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['All', 'work', 'play', 'makes', 'jack', 'dull', 'boy', '.']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get nltk stopwords \n",
    "stops = set(stopwords.words('english'))\n",
    "\n",
    "# verify that we can pipeline these together. \n",
    "[word for word in word_tokenize(\"All work and no play makes jack a dull boy.\") if word not in stops]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zachary/opt/anaconda3/envs/eecs_nlp/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:396: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", 'could', 'might', 'must', \"n't\", 'need', 'sha', 'wo', 'would'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<8042x72904 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 3624741 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now vectorize our traain set... \n",
    "vectorizer = CountVectorizer(lowercase=True,\n",
    "\t\t\t\t\t\t\ttokenizer=word_tokenize,\n",
    "\t\t\t\t\t\t\tstop_words=stops,\n",
    "\t\t\t\t\t\t\tngram_range=(1,3),    \t# might want to hyperparameter tune this\n",
    "\t\t\t\t\t\t\tmax_df=0.9,\t\t\t\t# might want to hyperparameter tune this \n",
    "\t\t\t\t\t\t\tmin_df=0.001,\t\t\t\t# might want to hyperparameter tune this\n",
    "\t\t\t\t\t\t\t)\n",
    "X_train = vectorizer.fit_transform(train_df[\"text\"].to_list())\n",
    "X_eval = vectorizer.transform(eval_df[\"text\"].to_list())\n",
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GaussianNB()"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now create our fun lil naieve bayes model\n",
    "classifier = GaussianNB()\n",
    "classifier.fit(X_train.toarray(),train_df['propaganda_label'].to_numpy())\n",
    "\n",
    "classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>score</th>\n",
       "      <td>0.812592</td>\n",
       "      <td>0.965217</td>\n",
       "      <td>0.882353</td>\n",
       "      <td>0.871304</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Precision    recall        f1  Accuracy\n",
       "score   0.812592  0.965217  0.882353  0.871304"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for uni-bi-gram models\n",
    "preds = classifier.predict( X_eval.toarray()) \n",
    "eval_df[\"preds\"] = preds \n",
    "prec,recall,f_1,sup = precision_recall_fscore_support(eval_df.propaganda_label,eval_df.preds)\n",
    "accuracy = accuracy_score(eval_df.propaganda_label,eval_df.preds)\n",
    "stats_arr = {\"Precision\": prec[1], \"recall\" : recall[1], \"f1\" : f_1[1],\"Accuracy\": accuracy}\n",
    "stats = pd.DataFrame(stats_arr, ['score'])\n",
    "stats\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zachary/opt/anaconda3/envs/eecs_nlp/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:396: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", 'could', 'might', 'must', \"n't\", 'need', 'sha', 'wo', 'would'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<8042x4846 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 1684269 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#unigram naieve bayses\n",
    "vectorizer = CountVectorizer(lowercase=True,\n",
    "\t\t\t\t\t\t\ttokenizer=word_tokenize,\n",
    "\t\t\t\t\t\t\tstop_words=stops,\n",
    "\t\t\t\t\t\t\tngram_range=(1,1),    \t# might want to hyperparameter tune this\n",
    "\t\t\t\t\t\t\tmax_df=1.0,\t\t\t\t# might want to hyperparameter tune this \n",
    "\t\t\t\t\t\t\tmin_df=0.01,\t\t\t\t# might want to hyperparameter tune this\n",
    "\t\t\t\t\t\t\t)\n",
    "X_train_simp = vectorizer.fit_transform(train_df[\"text\"].to_list())\n",
    "X_eval_simp = vectorizer.transform(eval_df[\"text\"].to_list())\n",
    "X_train_simp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GaussianNB()"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now create our fun lil naieve bayes model\n",
    "classifier = GaussianNB()\n",
    "classifier.fit(X_train_simp.toarray(),train_df['propaganda_label'].to_numpy())\n",
    "\n",
    "classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train</th>\n",
       "      <th>test</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Precision</th>\n",
       "      <td>0.902741</td>\n",
       "      <td>0.837736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>recall</th>\n",
       "      <td>0.761751</td>\n",
       "      <td>0.772174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f1</th>\n",
       "      <td>0.826275</td>\n",
       "      <td>0.803620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Accuracy</th>\n",
       "      <td>0.839841</td>\n",
       "      <td>0.811304</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              train      test\n",
       "Precision  0.902741  0.837736\n",
       "recall     0.761751  0.772174\n",
       "f1         0.826275  0.803620\n",
       "Accuracy   0.839841  0.811304"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = classifier.predict( X_eval_simp.toarray()) \n",
    "eval_df[\"preds\"] = preds \n",
    "prec,recall,f_1,sup = precision_recall_fscore_support(eval_df.propaganda_label,eval_df.preds)\n",
    "accuracy = accuracy_score(eval_df.propaganda_label,eval_df.preds)\n",
    "stats_arr = {\"Precision\": prec[1], \"recall\" : recall[1], \"f1\" : f_1[1],\"Accuracy\": accuracy}\n",
    "\n",
    "train_preds = classifier.predict( X_train_simp.toarray()) \n",
    "train_df['preds'] = train_preds\n",
    "prec,recall,f_1,sup = precision_recall_fscore_support(train_df.propaganda_label,train_df.preds)\n",
    "accuracy = accuracy_score(train_df.propaganda_label,train_df.preds)\n",
    "stats_arr_train = {\"Precision\": prec[1], \"recall\" : recall[1], \"f1\" : f_1[1],\"Accuracy\": accuracy}\n",
    "\n",
    "\n",
    "stats = pd.DataFrame([stats_arr_train,stats_arr], ['train','test'])\n",
    "stats.transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "28584a04334ae809113f3de0b099234fabcb50b20581a378ae9cc73c9f7a5983"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

***** Running training *****
  Num examples = 1848
  Num Epochs = 20
  Instantaneous batch size per device = 8
  Total train batch size (w. parallel, distributed & accumulation) = 8
  Gradient Accumulation steps = 1
  Total optimization steps = 4620
drive/My Drive/Colab Notebooks/Project/dataset.py:70: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  item = {key: torch.tensor(val[idx]) for key, val in self.encoding.items()}
 [4620/4620 09:42, Epoch 20/20]
Epoch	Training Loss	Validation Loss	Accuracy	F1	Precision	Recall
1	No log	2.555801	0.063333	0.009163	0.004872	0.076923
2	No log	2.533067	0.103333	0.072969	0.143283	0.120877
3	2.451900	2.511568	0.200000	0.138459	0.163566	0.159317
4	2.451900	2.681967	0.190000	0.122177	0.129682	0.162203
5	1.760300	2.864723	0.196667	0.161401	0.170293	0.175635
6	1.760300	2.964809	0.193333	0.151667	0.154235	0.166876
7	1.051900	3.202821	0.170000	0.142631	0.147986	0.148001
8	1.051900	3.376514	0.190000	0.162814	0.172275	0.165090
9	0.558000	3.600565	0.180000	0.157802	0.166322	0.162308
10	0.558000	3.839206	0.180000	0.151179	0.156698	0.158331
11	0.280200	4.027568	0.186667	0.155946	0.162621	0.163501
12	0.280200	4.304854	0.176667	0.152321	0.154733	0.165092
13	0.140600	4.579423	0.173333	0.147875	0.152396	0.158179
14	0.140600	4.717809	0.186667	0.155327	0.165589	0.163223
15	0.140600	4.826795	0.183333	0.155399	0.164204	0.164610
16	0.078900	4.948582	0.183333	0.157752	0.178063	0.167853
17	0.078900	5.051415	0.186667	0.162108	0.177421	0.170354
18	0.050000	5.149923	0.173333	0.149722	0.163500	0.157542
19	0.050000	5.154203	0.186667	0.161875	0.174480	0.170354
20	0.039600	5.184128	0.186667	0.162586	0.180257	0.170354

***** Running training *****
  Num examples = 1848
  Num Epochs = 20
  Instantaneous batch size per device = 8
  Total train batch size (w. parallel, distributed & accumulation) = 8
  Gradient Accumulation steps = 1
  Total optimization steps = 4620
drive/My Drive/Colab Notebooks/Project/dataset.py:70: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  item = {key: torch.tensor(val[idx]) for key, val in self.encoding.items()}
 [4620/4620 10:19, Epoch 20/20]
Epoch	Training Loss	Validation Loss	Accuracy	F1	Precision	Recall
1	No log	2.484853	0.173333	0.119654	0.178566	0.161929
2	No log	2.786729	0.186667	0.153308	0.160843	0.149668
3	1.933200	3.829186	0.176667	0.152600	0.164532	0.152166
4	1.933200	4.577184	0.150000	0.136259	0.146928	0.144116
5	0.633400	6.101936	0.156667	0.128873	0.126323	0.142270
6	0.633400	6.540385	0.180000	0.153654	0.159923	0.161396
7	0.184000	7.074298	0.180000	0.149453	0.151168	0.161126
8	0.184000	7.242388	0.206667	0.153640	0.150451	0.172602
9	0.051600	7.632096	0.186667	0.151252	0.157326	0.163200
10	0.051600	7.807064	0.176667	0.148303	0.154855	0.153603
11	0.019600	7.958395	0.190000	0.151075	0.155947	0.162028
12	0.019600	8.060586	0.190000	0.143944	0.139282	0.161314
13	0.013500	8.284497	0.186667	0.140513	0.138015	0.160551
14	0.013500	8.321315	0.190000	0.150890	0.152679	0.165076
15	0.013500	8.373812	0.180000	0.133507	0.125225	0.155017
16	0.011300	8.382454	0.186667	0.134307	0.128895	0.160803
17	0.011300	8.364845	0.183333	0.138191	0.135550	0.156278
18	0.008500	8.404383	0.186667	0.142961	0.143054	0.157539
19	0.008500	8.460641	0.190000	0.145358	0.144901	0.162064
20	0.007000	8.482163	0.190000	0.143275	0.142749	0.162064
***** Running Evaluation *****
  Num examples = 300
  Batch size = 8

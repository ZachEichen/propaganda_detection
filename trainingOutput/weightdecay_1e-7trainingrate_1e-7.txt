***** Running training *****
  Num examples = 1848
  Num Epochs = 20
  Instantaneous batch size per device = 8
  Total train batch size (w. parallel, distributed & accumulation) = 8
  Gradient Accumulation steps = 1
  Total optimization steps = 4620
drive/My Drive/Colab Notebooks/Project/dataset.py:70: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  item = {key: torch.tensor(val[idx]) for key, val in self.encoding.items()}
 [4620/4620 09:44, Epoch 20/20]
Epoch	Training Loss	Validation Loss	Accuracy	F1	Precision	Recall
1	No log	2.550941	0.073333	0.019743	0.014568	0.073030
2	No log	2.550266	0.080000	0.023818	0.091664	0.077496
3	2.575200	2.549502	0.083333	0.031197	0.062456	0.076714
4	2.575200	2.548628	0.086667	0.029717	0.063043	0.075339
5	2.569000	2.547866	0.100000	0.035383	0.053724	0.078856
6	2.569000	2.547115	0.090000	0.025078	0.016085	0.065275
7	2.561800	2.546533	0.080000	0.022004	0.013923	0.054331
8	2.561800	2.545876	0.096667	0.029980	0.019620	0.068057
9	2.555200	2.545268	0.103333	0.031436	0.037966	0.068050
10	2.555200	2.544774	0.100000	0.028537	0.018257	0.066789
11	2.551900	2.544216	0.106667	0.030500	0.019471	0.073557
12	2.551900	2.543718	0.103333	0.027744	0.017639	0.070352
13	2.546900	2.543267	0.106667	0.029669	0.019087	0.076573
14	2.546900	2.542984	0.113333	0.031501	0.020717	0.082498
15	2.546900	2.542729	0.106667	0.030179	0.020910	0.078746
16	2.542900	2.542543	0.110000	0.031425	0.022268	0.082794
17	2.542900	2.542395	0.113333	0.032504	0.022918	0.086843
18	2.539200	2.542287	0.110000	0.031946	0.023885	0.084967
19	2.539200	2.542224	0.110000	0.032091	0.025360	0.084967
20	2.538100	2.542211	0.110000	0.032091	0.025360	0.084967